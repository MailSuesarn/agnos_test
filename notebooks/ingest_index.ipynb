{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd68d135-6a6b-4797-8ced-9d2c1fd56895",
   "metadata": {},
   "source": [
    "# imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da3964-6c34-44f8-931b-84277a4eb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, unicodedata, pathlib, math\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from slugify import slugify\n",
    "import time, math\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from neo4j import GraphDatabase, basic_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572368b-af87-4596-af01-cf5d47dc48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Text cleanup (Thai + general) ----------\n",
    "_ws_re = re.compile(r\"[ \\t\\u00A0\\u200B\\u200C\\u200D\\u2060]+\")  # spaces + NBSP + ZW* + word joiner\n",
    "_nl_re = re.compile(r\"\\s*\\n\\s*\")\n",
    "def normalize_text(x: Optional[str]) -> str:\n",
    "    if x is None: \n",
    "        return \"\"\n",
    "    x = unicodedata.normalize(\"NFC\", str(x))\n",
    "    x = x.replace(\"\\r\", \"\\n\")\n",
    "    x = _nl_re.sub(\"\\n\", x).strip()\n",
    "    x = _ws_re.sub(\" \", x)\n",
    "    x = re.sub(r\"\\n[•\\-\\u2022]\\s*\", \"\\n• \", x)\n",
    "    x = re.sub(r\"\\n{3,}\", \"\\n\\n\", x)\n",
    "    return x\n",
    "\n",
    "def coalesce(*args, default=\"\"):\n",
    "    for a in args:\n",
    "        if a is not None and str(a).strip():\n",
    "            return str(a)\n",
    "    return default\n",
    "\n",
    "def ensure_list(x):\n",
    "    return x if isinstance(x, list) else [x]\n",
    "\n",
    "# ---------- JSON loader (array-of-objects) ----------\n",
    "def load_json(path_json: str) -> pd.DataFrame:\n",
    "    p = pathlib.Path(path_json)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{p} not found.\")\n",
    "    try:\n",
    "        # Expect a single JSON array of objects\n",
    "        return pd.read_json(p, orient=\"records\")\n",
    "    except ValueError:\n",
    "        # If you accidentally saved as JSON Lines, uncomment the next line:\n",
    "        # return pd.read_json(p, orient=\"records\", lines=True)\n",
    "        raise\n",
    "\n",
    "# ---------- Neo4j credentials loader ----------\n",
    "def load_neo4j_creds(from_txt: str) -> Dict[str, str]:\n",
    "    d = {}\n",
    "    with open(from_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"): \n",
    "                continue\n",
    "            if \"=\" in line:\n",
    "                k, v = line.split(\"=\", 1)\n",
    "                d[k.strip()] = v.strip()\n",
    "    req = [\"NEO4J_URI\",\"NEO4J_USERNAME\",\"NEO4J_PASSWORD\",\"NEO4J_DATABASE\"]\n",
    "    missing = [k for k in req if k not in d]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing keys in creds file: {missing}\")\n",
    "    return {\n",
    "        \"uri\": d[\"NEO4J_URI\"],\n",
    "        \"user\": d[\"NEO4J_USERNAME\"],\n",
    "        \"pwd\": d[\"NEO4J_PASSWORD\"],\n",
    "        \"db\": d[\"NEO4J_DATABASE\"],\n",
    "    }\n",
    "\n",
    "# ---------- Neo4j driver ----------\n",
    "class Neo4jClient:\n",
    "    def __init__(self, uri: str, user: str, pwd: str, database: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=basic_auth(user, pwd))\n",
    "        self.database = database\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    def run(self, cypher: str, **params):\n",
    "        with self.driver.session(database=self.database) as sess:\n",
    "            res = sess.run(cypher, params)\n",
    "            return list(res)\n",
    "\n",
    "def pcount(label: str, client: Neo4jClient):\n",
    "    out = client.run(f\"MATCH (n:{label}) RETURN count(n) AS c\")\n",
    "    return out[0][\"c\"] if out else 0\n",
    "\n",
    "\n",
    "def canonicalize_url(u: str) -> str:\n",
    "    p = urlparse(u.strip())\n",
    "    # lowercase scheme/host; drop params/query/fragment; strip trailing slash\n",
    "    new = p._replace(\n",
    "        scheme=p.scheme.lower(),\n",
    "        netloc=p.netloc.lower(),\n",
    "        params=\"\",\n",
    "        query=\"\",\n",
    "        fragment=\"\"\n",
    "    )\n",
    "    out = urlunparse(new).rstrip(\"/\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def embed_passages(texts, batch_size=64):\n",
    "    texts = [f\"passage: {t}\" if not str(t).lower().startswith(\"passage:\") else str(t) for t in texts]\n",
    "    return np.asarray(\n",
    "        model.encode(texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "\n",
    "def chunker(seq, n):\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i+n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df368c40-4239-4713-aed7-57bffe033f76",
   "metadata": {},
   "source": [
    "# load dataframes & normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa039670-22ec-47a2-b4f8-8bc1bf3f2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON paths only\n",
    "threads_json  = \"agnos_forum_threads.json\"\n",
    "diseases_json = \"agnos_diseases.json\"\n",
    "\n",
    "df_threads = load_json(threads_json)\n",
    "df_diseases = load_json(diseases_json)\n",
    "\n",
    "# --- Light cleanup / column harmonization ---\n",
    "\n",
    "# THREADS unified schema:\n",
    "# url, thread_id, title, thread_category, answer_by_doctor, title_category\n",
    "colmap_candidates = {\n",
    "    \"title\": [\"title\",\"question\",\"thread_title\",\"big_title\"],\n",
    "    \"thread_category\": [\"thread_category\",\"category\",\"chip_category\"],\n",
    "    \"answer_by_doctor\": [\"answer_by_doctor\",\"doctor_answer\",\"answer\",\"answer_text\"],\n",
    "    \"thread_id\": [\"thread_id\",\"slug\",\"id\"],\n",
    "    \"url\": [\"url\",\"link\",\"page_url\"]\n",
    "}\n",
    "def pick_col(df, keys):\n",
    "    for k in keys:\n",
    "        if k in df.columns: \n",
    "            return k\n",
    "    return None\n",
    "\n",
    "t_title_col = pick_col(df_threads, colmap_candidates[\"title\"])\n",
    "t_cat_col   = pick_col(df_threads, colmap_candidates[\"thread_category\"])\n",
    "t_ans_col   = pick_col(df_threads, colmap_candidates[\"answer_by_doctor\"])\n",
    "t_id_col    = pick_col(df_threads, colmap_candidates[\"thread_id\"])\n",
    "t_url_col   = pick_col(df_threads, colmap_candidates[\"url\"])\n",
    "\n",
    "if t_title_col is None or t_cat_col is None or t_url_col is None:\n",
    "    raise RuntimeError(\"Threads JSON missing required columns (title/category/url).\")\n",
    "\n",
    "def derive_thread_id(u: str) -> str:\n",
    "    try:\n",
    "        slug = u.strip(\"/\").split(\"/\")[-1]\n",
    "        m = re.search(r\"(\\d+)\", slug)\n",
    "        return m.group(1) if m else slugify(slug, lowercase=True)\n",
    "    except Exception:\n",
    "        return slugify(u, lowercase=True)\n",
    "\n",
    "threads = []\n",
    "for _, r in df_threads.iterrows():\n",
    "    url = canonicalize_url(normalize_text(r.get(t_url_col, \"\")))\n",
    "    title = normalize_text(r.get(t_title_col, \"\"))\n",
    "    thread_category = normalize_text(r.get(t_cat_col, \"\"))\n",
    "    answer_by_doctor = normalize_text(r.get(t_ans_col, \"\")) if t_ans_col else \"\"\n",
    "    thread_id = normalize_text(r.get(t_id_col, \"\")) if t_id_col else \"\"\n",
    "    if not thread_id:\n",
    "        thread_id = derive_thread_id(url)\n",
    "    title_category = f\"{title} {thread_category}\".strip()\n",
    "    threads.append({\n",
    "        \"url\": url,\n",
    "        \"thread_id\": thread_id,\n",
    "        \"title\": title,\n",
    "        \"thread_category\": thread_category,\n",
    "        \"answer_by_doctor\": answer_by_doctor or None,\n",
    "        \"title_category\": title_category\n",
    "    })\n",
    "df_threads_u = pd.DataFrame(threads)\n",
    "\n",
    "# DISEASES unified schema:\n",
    "# url, slug, thai_name, english_name, title, info_and_causes, symptoms, diagnosis, treatment,\n",
    "# specialist_doctor_recommended, precautions, additional_info, symptom_disease\n",
    "def find_first(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "d_url = find_first(df_diseases, [\"url\",\"page_url\",\"link\"])\n",
    "d_title = find_first(df_diseases, [\"title\",\"thai_title\",\"thai_name\"])\n",
    "d_en = find_first(df_diseases, [\"english_name\",\"en_name\",\"en_title\"])\n",
    "d_th = find_first(df_diseases, [\"thai_name\",\"thai_title\",\"title\"])\n",
    "d_info = find_first(df_diseases, [\"ข้อมูลและสาเหตุของโรค\",\"info_and_causes\",\"causes\",\"ข้อมูลสาเหตุ\"])\n",
    "d_symp = find_first(df_diseases, [\"อาการ\",\"symptoms\"])\n",
    "d_dx = find_first(df_diseases, [\"แนวทางการวินิจฉัย\",\"diagnosis\"])\n",
    "d_tx = find_first(df_diseases, [\"แนวทางการรักษา\",\"treatment\"])\n",
    "d_adv = find_first(df_diseases, [\"คำแนะนำจากผู้เชี่ยวชาญ\",\"specialist_doctor_recommended\",\"advice\"])\n",
    "d_warn = find_first(df_diseases, [\"ข้อควรระวัง\",\"precautions\",\"warning\"])\n",
    "d_more = find_first(df_diseases, [\"ข้อมูลเพิ่มเติม\",\"additional_info\",\"more_info\"])\n",
    "\n",
    "if d_url is None:\n",
    "    raise RuntimeError(\"Diseases JSON missing required column: url\")\n",
    "\n",
    "diseases = []\n",
    "for _, r in df_diseases.iterrows():\n",
    "    # url = normalize_text(r.get(d_url, \"\"))\n",
    "    url = canonicalize_url(normalize_text(r.get(d_url, \"\")))\n",
    "    slug = slugify(url.strip(\"/\").split(\"/\")[-1], lowercase=True)\n",
    "    thai_name = normalize_text(r.get(d_th, \"\")) if d_th else \"\"\n",
    "    english_name = normalize_text(r.get(d_en, \"\")) if d_en else \"\"\n",
    "    title = normalize_text(r.get(d_title, \"\")) if d_title else (thai_name or english_name)\n",
    "    info_and_causes = normalize_text(r.get(d_info, \"\")) if d_info else \"\"\n",
    "    symptoms = normalize_text(r.get(d_symp, \"\")) if d_symp else \"\"\n",
    "    diagnosis = normalize_text(r.get(d_dx, \"\")) if d_dx else \"\"\n",
    "    treatment = normalize_text(r.get(d_tx, \"\")) if d_tx else \"\"\n",
    "    specialist = normalize_text(r.get(d_adv, \"\")) if d_adv else \"\"\n",
    "    precautions = normalize_text(r.get(d_warn, \"\")) if d_warn else \"\"\n",
    "    additional_info = normalize_text(r.get(d_more, \"\")) if d_more else \"\"\n",
    "    symptom_disease = f\"{info_and_causes} {symptoms}\".strip()\n",
    "    diseases.append({\n",
    "        \"url\": url,\n",
    "        \"slug\": slug,\n",
    "        \"thai_name\": thai_name or title,\n",
    "        \"english_name\": english_name or \"\",\n",
    "        \"title\": title,\n",
    "        \"info_and_causes\": info_and_causes,\n",
    "        \"symptoms\": symptoms,\n",
    "        \"diagnosis\": diagnosis,\n",
    "        \"treatment\": treatment,\n",
    "        \"specialist_doctor_recommended\": specialist,\n",
    "        \"precautions\": precautions,\n",
    "        \"additional_info\": additional_info,\n",
    "        \"symptom_disease\": symptom_disease\n",
    "    })\n",
    "df_diseases_u = pd.DataFrame(diseases)\n",
    "\n",
    "print(\"Threads:\", df_threads_u.shape, \"Diseases:\", df_diseases_u.shape)\n",
    "df_threads_u.head(2), df_diseases_u.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2ee25-6b4d-43f6-92a1-e144df5e221e",
   "metadata": {},
   "source": [
    "# Dev Mode for faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724c4fa-6546-4af3-914f-a10e52308371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DEV MODE: fast subset without renaming downstream vars =====\n",
    "DEV_MODE   = False          # flip to False to run on full data\n",
    "N_THREADS  = 20           # quick dev size\n",
    "N_DISEASES = 10           # quick dev size\n",
    "\n",
    "def _subset(df, text_col, n):\n",
    "    d = df.copy()\n",
    "    # keep rows that actually have some text\n",
    "    d = d[d[text_col].astype(str).str.len() >= 6]\n",
    "    # de-dup by URL just in case\n",
    "    if \"url\" in d.columns:\n",
    "        d = d.drop_duplicates(subset=[\"url\"])\n",
    "    # deterministic sample if larger than n\n",
    "    if len(d) > n:\n",
    "        d = d.sample(n=n, random_state=42)\n",
    "    return d.reset_index(drop=True)\n",
    "\n",
    "if DEV_MODE:\n",
    "    _orig_t, _orig_d = len(df_threads_u), len(df_diseases_u)\n",
    "    df_threads_u  = _subset(df_threads_u,  \"title_category\",   N_THREADS)\n",
    "    df_diseases_u = _subset(df_diseases_u, \"symptom_disease\",  N_DISEASES)\n",
    "    print(f\"[DEV_MODE] Threads {len(df_threads_u)}/{_orig_t} | Diseases {len(df_diseases_u)}/{_orig_d}\")\n",
    "else:\n",
    "    print(\"[DEV_MODE] OFF — using full datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1112675-f576-425a-a9ac-e44316db2da7",
   "metadata": {},
   "source": [
    "# connect to Neo4j (reads creds from the provided TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1074c71-668a-454f-ab14-e9f30aa60754",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDS_TXT = \"Neo4j-92e8f832-Created-2025-08-28.txt\" \n",
    "creds = load_neo4j_creds(CREDS_TXT)\n",
    "client = Neo4jClient(creds[\"uri\"], creds[\"user\"], creds[\"pwd\"], creds[\"db\"])\n",
    "\n",
    "print(\"Connected to:\", creds[\"uri\"], \" DB:\", creds[\"db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5ebec-2b2a-440b-a292-9e4f7ea8f220",
   "metadata": {},
   "source": [
    "# embedding model (BAAI/bge-m3) & dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d1a3b-d660-4dad-83e3-22eef1f66e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (1024-d). We'll normalize (L2) for cosine similarity.\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model = SentenceTransformer(model_name)\n",
    "emb_dim = model.get_sentence_embedding_dimension()\n",
    "print(\"Model:\", model_name, \"Embedding dim:\", emb_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd0fd3-c3fe-46ef-ae73-2f9382cf5540",
   "metadata": {},
   "source": [
    "# upsert Threads with embeddings (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93e2bf-615b-43b9-a493-8035c11c3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-up (no-op if you've run it already)\n",
    "_ = model.encode([\"query: warmup\"], normalize_embeddings=True)\n",
    "\n",
    "# Uniqueness constraint (idempotent)\n",
    "client.run(\"CREATE CONSTRAINT thread_url_unique IF NOT EXISTS FOR (t:Thread) REQUIRE t.url IS UNIQUE\")\n",
    "\n",
    "# Prepare rows from df (re-canonicalize defensively even if you canonicalized at load time)\n",
    "rows = [{\n",
    "    \"url\": canonicalize_url(r[\"url\"]),\n",
    "    \"thread_id\": r[\"thread_id\"],\n",
    "    \"title\": r[\"title\"],\n",
    "    \"thread_category\": r[\"thread_category\"],\n",
    "    \"answer_by_doctor\": r.get(\"answer_by_doctor\"),\n",
    "    \"title_category\": r[\"title_category\"],\n",
    "} for r in df_threads_u.to_dict(\"records\")]\n",
    "\n",
    "# Find existing Thread nodes that already have embeddings\n",
    "existing_rows = client.run(\"\"\"\n",
    "MATCH (t:Thread)\n",
    "WHERE t.title_category_emb IS NOT NULL\n",
    "RETURN t.url AS url\n",
    "\"\"\")\n",
    "existing = { canonicalize_url(r[\"url\"]) for r in existing_rows }\n",
    "\n",
    "# Toggle whether to re-embed existing rows\n",
    "REEMBED_EXISTING = False  # set True to recompute embeddings for everything\n",
    "\n",
    "if REEMBED_EXISTING:\n",
    "    to_embed = rows\n",
    "    to_update_only = []\n",
    "else:\n",
    "    to_embed = [r for r in rows if r[\"url\"] not in existing]\n",
    "    to_update_only = [r for r in rows if r[\"url\"] in existing]\n",
    "\n",
    "print(f\"Threads to embed: {len(to_embed)} | to update-only: {len(to_update_only)}\")\n",
    "\n",
    "# --- Update-only (refresh scalar fields, keep existing embedding) ---\n",
    "if to_update_only:\n",
    "    cypher_update_only = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (t:Thread {url: row.url})\n",
    "    SET  t.thread_id = row.thread_id,\n",
    "         t.title = row.title,\n",
    "         t.thread_category = row.thread_category,\n",
    "         t.answer_by_doctor = row.answer_by_doctor,\n",
    "         t.title_category = row.title_category\n",
    "    \"\"\"\n",
    "    BATCH = 64\n",
    "    for bi, part in enumerate(chunker(to_update_only, BATCH), start=1):\n",
    "        t0 = time.perf_counter()\n",
    "        client.run(cypher_update_only, rows=part)\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"[update-only] Batch {bi} — {len(part)} rows | write {t1-t0:.1f}s\")\n",
    "\n",
    "# --- Embed + upsert (set embedding for new/changed items) ---\n",
    "if to_embed:\n",
    "    cypher_embed = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (t:Thread {url: row.url})\n",
    "    SET  t.thread_id = row.thread_id,\n",
    "         t.title = row.title,\n",
    "         t.thread_category = row.thread_category,\n",
    "         t.answer_by_doctor = row.answer_by_doctor,\n",
    "         t.title_category = row.title_category,\n",
    "         t.title_category_emb = row.title_category_emb\n",
    "    \"\"\"\n",
    "    BATCH = 64  # small for responsive logs; increase later\n",
    "    total = len(to_embed)\n",
    "    num_batches = math.ceil(total / BATCH)\n",
    "    print(f\"Embedding & upserting {total} Thread rows in {num_batches} batches (BATCH={BATCH})\")\n",
    "    for bi, part in enumerate(chunker(to_embed, BATCH), start=1):\n",
    "        t0 = time.perf_counter()\n",
    "        embs = embed_passages([x[\"title_category\"] for x in part], batch_size=min(64, len(part)))\n",
    "        for i, x in enumerate(part):\n",
    "            x[\"title_category_emb\"] = embs[i].tolist()\n",
    "        t1 = time.perf_counter()\n",
    "        client.run(cypher_embed, rows=part)\n",
    "        t2 = time.perf_counter()\n",
    "        print(f\"[embed] Batch {bi}/{num_batches} — {len(part)} rows | embed {t1-t0:.1f}s | write {t2-t1:.1f}s\")\n",
    "\n",
    "print(\"Thread nodes now:\", pcount(\"Thread\", client))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c6786-8112-4fc5-a81b-43df40d66cc2",
   "metadata": {},
   "source": [
    "# upsert Diseases with embeddings (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d5f8f-79e1-44a5-81a9-6fab99eacb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness constraint (idempotent)\n",
    "client.run(\"CREATE CONSTRAINT disease_url_unique IF NOT EXISTS FOR (d:Disease) REQUIRE d.url IS UNIQUE\")\n",
    "\n",
    "# Prepare rows from dataframe (URLs are already canonicalized from step #2, but we re-canonicalize defensively)\n",
    "rows = [{\n",
    "    \"url\": canonicalize_url(r[\"url\"]),\n",
    "    \"slug\": r[\"slug\"],\n",
    "    \"thai_name\": r[\"thai_name\"],\n",
    "    \"english_name\": r.get(\"english_name\",\"\"),\n",
    "    \"title\": r[\"title\"],\n",
    "    \"info_and_causes\": r[\"info_and_causes\"],\n",
    "    \"symptoms\": r[\"symptoms\"],\n",
    "    \"diagnosis\": r[\"diagnosis\"],\n",
    "    \"treatment\": r[\"treatment\"],\n",
    "    \"specialist_doctor_recommended\": r[\"specialist_doctor_recommended\"],\n",
    "    \"precautions\": r[\"precautions\"],\n",
    "    \"additional_info\": r[\"additional_info\"],\n",
    "    \"symptom_disease\": r[\"symptom_disease\"],\n",
    "} for r in df_diseases_u.to_dict(\"records\")]\n",
    "\n",
    "# Skip diseases that already have embeddings in the DB (compare using canonical URLs)\n",
    "existing_rows = client.run(\"\"\"\n",
    "MATCH (d:Disease)\n",
    "WHERE d.symptom_disease_emb IS NOT NULL\n",
    "RETURN d.url AS url\n",
    "\"\"\")\n",
    "existing = { canonicalize_url(r[\"url\"]) for r in existing_rows }\n",
    "\n",
    "before = len(rows)\n",
    "rows = [r for r in rows if r[\"url\"] not in existing]\n",
    "skipped = before - len(rows)\n",
    "print(f\"Skipping {skipped} already-embedded Disease nodes; processing {len(rows)} new/changed.\")\n",
    "\n",
    "# Nothing to do?\n",
    "if not rows:\n",
    "    print(\"No Disease rows to upsert. (All have embeddings.)\")\n",
    "else:\n",
    "    BATCH = 16  # small batch for responsive logs (tweak as you like)\n",
    "    total = len(rows)\n",
    "    num_batches = math.ceil(total / BATCH)\n",
    "\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (d:Disease {url: row.url})\n",
    "    SET  d.slug = row.slug,\n",
    "         d.thai_name = row.thai_name,\n",
    "         d.english_name = row.english_name,\n",
    "         d.title = row.title,\n",
    "         d.info_and_causes = row.info_and_causes,\n",
    "         d.symptoms = row.symptoms,\n",
    "         d.diagnosis = row.diagnosis,\n",
    "         d.treatment = row.treatment,\n",
    "         d.specialist_doctor_recommended = row.specialist_doctor_recommended,\n",
    "         d.precautions = row.precautions,\n",
    "         d.additional_info = row.additional_info,\n",
    "         d.symptom_disease = row.symptom_disease,\n",
    "         d.symptom_disease_emb = row.symptom_disease_emb\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting upsert of {total} Disease rows in {num_batches} batches (BATCH={BATCH})\")\n",
    "    for bi, part in enumerate((rows[i:i+BATCH] for i in range(0, total, BATCH)), start=1):\n",
    "        t0 = time.perf_counter()\n",
    "        embs = embed_passages([x[\"symptom_disease\"] for x in part], batch_size=min(64, len(part)))\n",
    "        for i, x in enumerate(part):\n",
    "            x[\"symptom_disease_emb\"] = embs[i].tolist()\n",
    "        t1 = time.perf_counter()\n",
    "        client.run(cypher, rows=part)\n",
    "        t2 = time.perf_counter()\n",
    "        print(f\"Batch {bi}/{num_batches} — {len(part)} rows | embed {t1-t0:.1f}s | write {t2-t1:.1f}s\")\n",
    "\n",
    "print(\"Disease nodes now:\", pcount(\"Disease\", client))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd31db-9459-4c93-9302-b0b4d2b5f00a",
   "metadata": {},
   "source": [
    "# create vector indexes (native KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b855eb-ff50-426b-a431-ce5f789fbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create native VECTOR INDEXes (Aura supports this; 5.x+)\n",
    "client.run(f\"\"\"\n",
    "CREATE VECTOR INDEX thread_titlecat_idx IF NOT EXISTS\n",
    "FOR (t:Thread) ON (t.title_category_emb)\n",
    "OPTIONS {{\n",
    "  indexConfig: {{\n",
    "    `vector.dimensions`: $dim,\n",
    "    `vector.similarity_function`: 'cosine'\n",
    "  }}\n",
    "}}\n",
    "\"\"\", dim=int(emb_dim))\n",
    "\n",
    "client.run(f\"\"\"\n",
    "CREATE VECTOR INDEX disease_symptom_idx IF NOT EXISTS\n",
    "FOR (d:Disease) ON (d.symptom_disease_emb)\n",
    "OPTIONS {{\n",
    "  indexConfig: {{\n",
    "    `vector.dimensions`: $dim,\n",
    "    `vector.similarity_function`: 'cosine'\n",
    "  }}\n",
    "}}\n",
    "\"\"\", dim=int(emb_dim))\n",
    "\n",
    "print(\"Vector indexes created (or already exist).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa360cb-91f8-4936-9161-07ed9d2b4d18",
   "metadata": {},
   "source": [
    "# sanity checks (counts + small samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912e341-2138-48f7-8f33-a1938a478b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threads:\", pcount(\"Thread\", client))\n",
    "print(\"Diseases:\", pcount(\"Disease\", client))\n",
    "\n",
    "res_t = client.run(\"MATCH (t:Thread) RETURN t.url AS url, t.title AS title LIMIT 3\")\n",
    "res_d = client.run(\"MATCH (d:Disease) RETURN d.url AS url, d.title AS title LIMIT 3\")\n",
    "\n",
    "print(\"\\nSample Threads:\")\n",
    "for r in res_t:\n",
    "    print(\"-\", r[\"title\"], \"->\", r[\"url\"])\n",
    "\n",
    "print(\"\\nSample Diseases:\")\n",
    "for r in res_d:\n",
    "    print(\"-\", r[\"title\"], \"->\", r[\"url\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf7e08-6797-46de-9ffa-1d4984f5032c",
   "metadata": {},
   "source": [
    "# test KNN: threads (symptom/query) & diseases (name/query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fcb6e-c605-403c-89e9-3412e495c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(q: str) -> List[float]:\n",
    "    # Recommended query prefix for bge\n",
    "    q_prep = f\"query: {q}\" if not q.lower().startswith(\"query:\") else q\n",
    "    v = model.encode([q_prep], normalize_embeddings=True)[0].astype(np.float32).tolist()\n",
    "    return v\n",
    "\n",
    "def knn_threads(query: str, k: int = 5):\n",
    "    v = embed_query(query)\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('thread_titlecat_idx', $k, $v) \n",
    "    YIELD node, score\n",
    "    RETURN node.url AS url, node.title AS title, node.thread_category AS category, score\n",
    "    \"\"\"\n",
    "    return client.run(cypher, k=int(k), v=v)\n",
    "\n",
    "def knn_diseases(query: str, k: int = 2):\n",
    "    v = embed_query(query)\n",
    "    cypher = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('disease_symptom_idx', $k, $v)\n",
    "    YIELD node, score\n",
    "    RETURN node.url AS url, node.title AS title, node.thai_name AS th_name, node.english_name AS en_name, score\n",
    "    \"\"\"\n",
    "    return client.run(cypher, k=int(k), v=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb95e8-62a5-4b12-bf9c-1b6b260fcda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_symptoms = \"ผดผื่นที่แขน\"\n",
    "q_disease  = \"โรคไหล่อักเสบ\"  # e.g., user types a disease name\n",
    "\n",
    "print(\"=== THREADS by symptom/query ===\")\n",
    "for r in knn_threads(q_symptoms, k=5):\n",
    "    print(f\"{r['score']:.4f}\", \"-\", r[\"title\"], f\"({r.get('category')})\", \"->\", r[\"url\"])\n",
    "\n",
    "print(\"\\n=== DISEASES by disease-name/query ===\")\n",
    "for r in knn_diseases(q_disease, k=2):\n",
    "    en = r.get(\"en_name\") or \"\"\n",
    "    print(f\"{r['score']:.4f}\", \"-\", r[\"th_name\"], (f\"/ {en}\" if en else \"\"), \"->\", r[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9b637-0b26-4b2b-9e45-602337ce93bb",
   "metadata": {},
   "source": [
    "# (optional) compact counts of embedded nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10480e-752d-489e-b77b-6c64d8ccd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.run(\"\"\"\n",
    "MATCH (t:Thread) WHERE t.title_category_emb IS NOT NULL\n",
    "RETURN count(t) AS c_threads\n",
    "\"\"\")\n",
    "print(\"Threads with embeddings:\", res[0][\"c_threads\"])\n",
    "\n",
    "res = client.run(\"\"\"\n",
    "MATCH (d:Disease) WHERE d.symptom_disease_emb IS NOT NULL\n",
    "RETURN count(d) AS c_diseases\n",
    "\"\"\")\n",
    "print(\"Diseases with embeddings:\", res[0][\"c_diseases\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ae46e-11f6-4a20-a276-bbfacc1c87f4",
   "metadata": {},
   "source": [
    "# (cleanup helper) close driver when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d933093-f062-40e4-88c5-1618e6c32d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "print(\"Closed Neo4j driver.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d22e5-f8d0-46b4-b0d5-b8e77abecddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
