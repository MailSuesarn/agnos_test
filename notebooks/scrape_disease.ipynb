{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83976842-e5a4-4e02-831e-c1eb4a92aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, json, urllib.parse\n",
    "from typing import Dict, Optional, List\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Try Chrome first (Selenium Manager), then Edge (usually present on Windows)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "\n",
    "HEADINGS_MAP = {\n",
    "    \"ข้อมูลและสาเหตุของโรค\": \"info_and_causes\",\n",
    "    \"ข้อมูลโรคและสาเหตุการเกิดโรค\": \"info_and_causes\",  # variant\n",
    "    \"อาการของโรค\": \"symptoms\",\n",
    "    \"แนวทางการตรวจวินิจฉัยโรค\": \"diagnosis\",\n",
    "    \"แนวทางการดูแลรักษา\": \"treatment\",\n",
    "    \"แพทย์เฉพาะทางแนะนำ\": \"specialist_doctor_recommended\",\n",
    "    \"ข้อควรระวัง\": \"precautions\",\n",
    "    \"ข้อมูลเพิ่มเติม\": \"additional_info\",\n",
    "}\n",
    "\n",
    "def _clean(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None: return None\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s or None\n",
    "\n",
    "def _collect_by_siblings(start_el: Tag, heading_texts: set) -> Optional[str]:\n",
    "    \"\"\"Primary strategy: walk next siblings until the next recognized heading.\"\"\"\n",
    "    parts: List[str] = []\n",
    "    cur = start_el.next_sibling\n",
    "    while cur:\n",
    "        if isinstance(cur, NavigableString):\n",
    "            cur = cur.next_sibling\n",
    "            continue\n",
    "        if isinstance(cur, Tag):\n",
    "            t_all = _clean(cur.get_text(\" \", strip=True))\n",
    "            if t_all in heading_texts:\n",
    "                break\n",
    "            if cur.name in (\"p\",\"div\") and t_all:\n",
    "                parts.append(t_all)\n",
    "            elif cur.name in (\"ul\",\"ol\"):\n",
    "                for li in cur.find_all(\"li\"):\n",
    "                    lt = _clean(li.get_text(\" \", strip=True))\n",
    "                    if lt: parts.append(\"• \" + lt)\n",
    "        cur = cur.next_sibling\n",
    "    return _clean(\" \".join(parts)) if parts else None\n",
    "\n",
    "def _collect_by_walkdown(start_el: Tag, heading_texts: set, max_nodes: int = 300) -> Optional[str]:\n",
    "    \"\"\"Fallback strategy: walk forward through the DOM (depth-first) until next heading is encountered.\"\"\"\n",
    "    parts: List[str] = []\n",
    "    count = 0\n",
    "    for node in start_el.find_all_next(True):\n",
    "        count += 1\n",
    "        if count > max_nodes: break\n",
    "        t_all = _clean(node.get_text(\" \", strip=True))\n",
    "        if not t_all: \n",
    "            continue\n",
    "        if t_all in heading_texts:\n",
    "            break\n",
    "        if node.name in (\"p\",\"div\"):\n",
    "            parts.append(t_all)\n",
    "        elif node.name in (\"ul\",\"ol\"):\n",
    "            for li in node.find_all(\"li\"):\n",
    "                lt = _clean(li.get_text(\" \", strip=True))\n",
    "                if lt: parts.append(\"• \" + lt)\n",
    "    return _clean(\" \".join(parts)) if parts else None\n",
    "\n",
    "def _open_driver(headless: bool = True):\n",
    "    # Try Chrome\n",
    "    try:\n",
    "        copts = ChromeOptions()\n",
    "        if headless: copts.add_argument(\"--headless=new\")\n",
    "        copts.add_argument(\"--disable-gpu\")\n",
    "        copts.add_argument(\"--lang=th-TH\")\n",
    "        copts.add_argument(\"--window-size=1200,2000\")\n",
    "        return webdriver.Chrome(options=copts)\n",
    "    except WebDriverException:\n",
    "        # Fallback to Edge (installed by default on Windows)\n",
    "        eopts = EdgeOptions()\n",
    "        if headless: eopts.add_argument(\"--headless=new\")\n",
    "        eopts.add_argument(\"--disable-gpu\")\n",
    "        eopts.add_argument(\"--lang=th-TH\")\n",
    "        eopts.add_argument(\"--window-size=1200,2000\")\n",
    "        return webdriver.Edge(options=eopts)\n",
    "\n",
    "def parse_disease_page_selenium(url: str, wait_sec: int = 20, headless: bool = True, debug: bool=False) -> Dict:\n",
    "    driver = _open_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for ANY known section heading to appear (robust to tag type/class)\n",
    "        any_heading_xpath = \" | \".join(\n",
    "            [f\"//*[normalize-space(text())='{h}']\" for h in HEADINGS_MAP.keys()]\n",
    "        )\n",
    "        try:\n",
    "            WebDriverWait(driver, wait_sec).until(\n",
    "                EC.presence_of_element_located((By.XPATH, any_heading_xpath))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            # Try a small scroll then one more wait\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1.0)\n",
    "            WebDriverWait(driver, 8).until(\n",
    "                EC.presence_of_element_located((By.XPATH, any_heading_xpath))\n",
    "            )\n",
    "\n",
    "        # Gentle scroll to trigger any lazy content\n",
    "        for _ in range(6):\n",
    "            driver.execute_script(\"window.scrollBy(0, 800);\"); time.sleep(0.25)\n",
    "\n",
    "        html = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Page title (best-effort)\n",
    "    title = None\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        title = _clean(h1.get_text(\" \", strip=True))\n",
    "\n",
    "    # Find heading nodes by visible text (not by classes)\n",
    "    heading_nodes: Dict[str, Tag] = {}\n",
    "    wanted = set(HEADINGS_MAP.keys())\n",
    "    for tag_name in (\"h2\",\"h3\",\"p\",\"div\",\"span\"):\n",
    "        for el in soup.find_all(tag_name):\n",
    "            t = _clean(el.get_text(\" \", strip=True))\n",
    "            if t and t in wanted and t not in heading_nodes:\n",
    "                heading_nodes[t] = el\n",
    "\n",
    "    out = {\n",
    "        \"url\": url,\n",
    "        \"slug\": urllib.parse.urlparse(url).path.rsplit(\"/\", 1)[-1],\n",
    "        \"title\": title,\n",
    "        \"info_and_causes\": None,\n",
    "        \"symptoms\": None,\n",
    "        \"diagnosis\": None,\n",
    "        \"treatment\": None,\n",
    "        \"specialist_doctor_recommended\": None,\n",
    "        \"precautions\": None,\n",
    "        \"additional_info\": None,\n",
    "    }\n",
    "\n",
    "    heading_texts = set(heading_nodes.keys())\n",
    "    for thai_heading, key in HEADINGS_MAP.items():\n",
    "        el = heading_nodes.get(thai_heading)\n",
    "        if not el:\n",
    "            continue\n",
    "        # Try sibling-walk first; fallback to forward-walk if empty\n",
    "        sec = _collect_by_siblings(el, heading_texts)\n",
    "        if not sec:\n",
    "            sec = _collect_by_walkdown(el, heading_texts)\n",
    "        if sec:\n",
    "            out[key] = sec\n",
    "\n",
    "    if debug:\n",
    "        print(json.dumps(out, ensure_ascii=False, indent=2))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de08ffa-c2d6-49e2-8960-c123ca501a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Discover diseases grouped by initial letter, then batch-scrape ----\n",
    "import re, json, csv, time, urllib.parse\n",
    "from typing import List, Dict, Optional\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "FORUM_BASE = \"https://www.agnoshealth.com\"\n",
    "INDEX_URL  = f\"{FORUM_BASE}/diseases-and-symptoms?tab=diseases&group=alphabet\"\n",
    "UA = {\"User-Agent\":\"AgnosRAGBot/1.1 (+demo)\", \"Accept-Language\":\"th,en;q=0.8\"}\n",
    "\n",
    "def _clean(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None: return None\n",
    "    return re.sub(r\"\\s+\",\" \", s).strip() or None\n",
    "\n",
    "def _slugify_en(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s\n",
    "\n",
    "def _build_url(thai_name: str, english_name: Optional[str]) -> Optional[str]:\n",
    "    if not thai_name: return None\n",
    "    if english_name:\n",
    "        thai_q   = urllib.parse.quote(thai_name, safe=\"\")\n",
    "        eng_slug = _slugify_en(english_name)\n",
    "        return f\"{FORUM_BASE}/diseases/{thai_q}/{eng_slug}\"\n",
    "    return None\n",
    "\n",
    "def _is_h2_letter(tag: Tag) -> bool:\n",
    "    if not isinstance(tag, Tag): return False\n",
    "    if tag.name != \"h2\": return False\n",
    "    classes = set(tag.get(\"class\", []))\n",
    "    return \"font-bold\" in classes and \"text-2xl\" in classes\n",
    "\n",
    "def _extract_grouped_from_html(html: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Walk each <h2.font-bold.text-2xl> (Thai initial letter),\n",
    "    then collect all <p.text-primary_blue-600> until the next <h2>.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items: List[Dict] = []\n",
    "\n",
    "    letters = [h for h in soup.find_all(\"h2\") if _is_h2_letter(h)]\n",
    "    for h2 in letters:\n",
    "        letter = _clean(h2.get_text(\" \", strip=True))\n",
    "        # walk siblings until next h2-letter\n",
    "        sib = h2.next_sibling\n",
    "        while sib and not (_is_h2_letter(sib) if isinstance(sib, Tag) else False):\n",
    "            if isinstance(sib, Tag):\n",
    "                for p in sib.find_all(\"p\", class_=\"text-primary_blue-600\"):\n",
    "                    text = _clean(p.get_text(\" \", strip=True))\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    # pattern: ไทย (English)\n",
    "                    m = re.match(r\"^(.*?)\\s*\\((.*?)\\)\\s*$\", text)\n",
    "                    thai    = m.group(1) if m else text\n",
    "                    english = m.group(2) if m else None\n",
    "\n",
    "                    # Prefer the real link if the <p> sits inside an <a> row\n",
    "                    href = None\n",
    "                    a = p.find_parent(\"a\")\n",
    "                    if a and a.get(\"href\"):\n",
    "                        href = urllib.parse.urljoin(FORUM_BASE, a[\"href\"])\n",
    "                    url = href or _build_url(thai, english)\n",
    "\n",
    "                    items.append({\"letter\": letter, \"thai\": thai, \"english\": english, \"url\": url})\n",
    "            sib = sib.next_sibling\n",
    "\n",
    "    # de-duplicate by URL (or (letter,thai,english) if URL missing)\n",
    "    seen = set(); unique = []\n",
    "    for it in items:\n",
    "        key = it[\"url\"] or (it[\"letter\"], it[\"thai\"], it.get(\"english\"))\n",
    "        if key in seen: \n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique.append(it)\n",
    "    return unique\n",
    "\n",
    "def discover_disease_links_grouped(headless: bool = True) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    HTTP first; if 0 found (JS-rendered), render with Selenium and retry.\n",
    "    Returns: [{letter, thai, english, url}]\n",
    "    \"\"\"\n",
    "    # Try plain HTTP\n",
    "    try:\n",
    "        with httpx.Client(headers=UA, timeout=30, follow_redirects=True) as c:\n",
    "            r = c.get(INDEX_URL)\n",
    "            if r.status_code == 200:\n",
    "                out = _extract_grouped_from_html(r.text)\n",
    "                if out:\n",
    "                    # quick per-letter stats\n",
    "                    by_letter = {}\n",
    "                    for x in out: by_letter.setdefault(x[\"letter\"], 0); by_letter[x[\"letter\"]] += 1\n",
    "                    print(f\"[discover] via HTTP: total {len(out)}\")\n",
    "                    print(\"  letters:\", \", \".join(f\"{k}:{v}\" for k,v in by_letter.items()))\n",
    "                    return out\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: Selenium render (we already have _open_driver from your parser)\n",
    "    print(\"[discover] HTTP yielded 0; rendering with Selenium …\")\n",
    "    driver = _open_driver(headless=headless)\n",
    "    try:\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "        driver.get(INDEX_URL)\n",
    "        # wait for any letter h2 and at least one disease row under it\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h2.font-bold.text-2xl\"))\n",
    "        )\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"p.text-primary_blue-600\"))\n",
    "        )\n",
    "        # scroll to ensure all groups load\n",
    "        for _ in range(10):\n",
    "            driver.execute_script(\"window.scrollBy(0, 1600);\")\n",
    "            time.sleep(0.25)\n",
    "\n",
    "        html = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    out = _extract_grouped_from_html(html)\n",
    "    by_letter = {}\n",
    "    for x in out: by_letter.setdefault(x[\"letter\"], 0); by_letter[x[\"letter\"]] += 1\n",
    "    print(f\"[discover] via Selenium: total {len(out)}\")\n",
    "    print(\"  letters:\", \", \".join(f\"{k}:{v}\" for k,v in by_letter.items()))\n",
    "    return out\n",
    "\n",
    "def scrape_diseases_to_files_grouped(limit: int = 0,\n",
    "                                     headless: bool = True,\n",
    "                                     json_path: str = \"agnos_diseases.json\",\n",
    "                                     csv_path: str = \"agnos_diseases.csv\") -> List[Dict]:\n",
    "    links = discover_disease_links_grouped(headless=headless)\n",
    "    if limit and limit > 0:\n",
    "        links = links[:limit]\n",
    "    print(f\"[scrape] will fetch {len(links)} disease pages\")\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for i, item in enumerate(links, 1):\n",
    "        url = item.get(\"url\")\n",
    "        if not url:\n",
    "            print(f\"[skip] no URL for: {item['letter']} / {item['thai']} ({item.get('english')})\")\n",
    "            continue\n",
    "        try:\n",
    "            row = parse_disease_page_selenium(url, headless=headless, debug=False)\n",
    "            # attach names from index\n",
    "            row[\"letter\"]       = item[\"letter\"]\n",
    "            row[\"thai_name\"]    = item[\"thai\"]\n",
    "            row[\"english_name\"] = item.get(\"english\")\n",
    "            rows.append(row)\n",
    "            print(f\"[{i}/{len(links)}] {item['letter']} • {item['thai']} ✓\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}/{len(links)}] FAIL {url}: {e}\")\n",
    "\n",
    "    # Save outputs\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    fields = [\"letter\",\"thai_name\",\"english_name\",\"url\",\"slug\",\"title\",\n",
    "              \"info_and_causes\",\"symptoms\",\"diagnosis\",\"treatment\",\n",
    "              \"specialist_doctor_recommended\",\"precautions\",\"additional_info\"]\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fields)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k) for k in fields})\n",
    "\n",
    "    print(f\"[done] saved {len(rows)} diseases → {json_path}, {csv_path}\")\n",
    "    if rows:\n",
    "        print(\"Sample:\")\n",
    "        print(json.dumps(rows[0], ensure_ascii=False, indent=2))\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c495d-48f4-409d-843b-b8e518304807",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = scrape_diseases_to_files_grouped(limit=0, headless=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe680e-d880-41f8-aed1-2f3a86f4b6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
